[{"id":0,"href":"/docs/compliance__regulatory_governance/","title":"Compliance \u0026 Regulatory Governance","section":"Docs","content":" Compliance \u0026amp; Regulatory Governance # Data Sovereignty \u0026amp; Localization # Impact of regulations (GDPR, HIPAA) on cloud adoption. # Tensions between global cloud providers and regional laws. # Industry-Specific Cloud Platforms # Compliance frameworks for healthcare, finance, and government sectors. # # "},{"id":1,"href":"/docs/container_registries/","title":"Container Registries","section":"Docs","content":" Container registries # Comprehensive Analysis: Security Risks of Exposed Container Registries and Compromised Docker Hub Accounts # [trendmicro.com]{.underline}\nThe integration of container technologies into modern development workflows has streamlined application deployment and scalability. However, as highlighted by Trend Micro, misconfigurations and inadequate security measures in container environments can introduce significant vulnerabilities.\nRisks Associated with Exposed Container Registries\nContainer registries function as centralized repositories for storing and distributing container images. When these registries are publicly accessible without proper authentication, they become susceptible to unauthorized access. Trend Micro emphasizes that such exposure can lead to:\nUnauthorized Data Access: Attackers can download proprietary software or sensitive data embedded within container images. This unauthorized access can result in data breaches and intellectual property theft.\nMalware Distribution: Malicious actors gaining access to exposed registries can upload compromised images. These images, once deployed, can disseminate malware across systems, posing significant security threats.\nCase Study: Compromised Docker Hub Accounts and Cryptomining\nIn October 2021, Trend Micro observed a campaign targeting misconfigured servers with exposed Docker REST APIs. Threat actors exploited these vulnerabilities by deploying containers from images designed to execute malicious scripts. These scripts facilitated unauthorized cryptocurrency mining, consuming substantial system resources and potentially leading to operational disruptions.\nThe group identified in this campaign, known as TeamTNT, has a history of abusing cloud environments for cryptomining activities. Their tactics include compromising Docker Hub accounts to distribute malicious images, further emphasizing the necessity for stringent security practices in container ecosystems.\nMitigation Strategies\nTo safeguard container environments from such threats, Trend Micro recommends the following measures:\nImplement Robust Authentication and Access Controls: Ensure that access to container registries and APIs is restricted to authorized personnel only. This prevents unauthorized interactions with container resources.\nRegular Monitoring and Logging: Continuously monitor container activities and maintain logs to detect and respond to suspicious behaviors promptly. Proactive monitoring aids in early threat detection and mitigation.\nNetwork Segmentation: Isolate container environments from other network segments to minimize potential attack surfaces. Proper segmentation restricts lateral movement of threats within the network.\nRegular Vulnerability Scanning: Conduct periodic scans of container images and registries to identify and remediate vulnerabilities. Regular assessments help maintain the integrity and security of containerized applications.\n"},{"id":2,"href":"/docs/core_security_challenges_in_cloud_environments/","title":"Core Security Challenges in Cloud Environments","section":"Docs","content":" Core Security Challenges in Cloud Environments # Case studies # Capital One (2019) # On July 17, 2019, Capital One's security team was alerted to a data leak by an email sent to their responsible disclosure box. A user reported that a GitHub repository contained suspicious commands to exfiltrate data from an AWS S3 storage belonging to the bank. Examination of these commands revealed a critical flaw: in just a few lines executed via the AWS CLI interface, an attacker could retrieve the authentication information of an IAM role, list all associated S3 buckets and massively download their contents. Log analysis showed that these commands had been executed several months earlier, potentially compromising the sensitive data of millions of customers.\nFurther investigation revealed the attack chain exploited. The attacker had exploited a Server-Side Request Forgery (SSRF) flaw on an exposed web server, a reverse proxy using ModSecurity, hosted on an AWS EC2 instance. By bypassing filtering rules, the attacker was able to query the EC2 instance's metadata service, an internal service that provides sensitive information, including the temporary credentials of IAM roles attached to the instance. Using these credentials, he was able to execute commands as this role and gain free access to the associated AWS resources.\nThe final link in the flaw lay in the misconfiguration of IAM permissions. By bypassing the filtering rules, the attacker was able to query the EC2 instance's metadata service, an internal service that provides sensitive information, including the temporary credentials of the IAM roles attached to the instance. Using these credentials, he was able to execute commands as this role and gain free access to the associated AWS resources.\nThe final link in the flaw lay in the misconfiguration of IAM permissions. The compromised role had excessive rights, including read access to all S3 buckets and the ability to decrypt protected data. This error enabled the attacker to exfiltrate around 30 GB of data, including millions of credit card applications, social security numbers and bank details. Once the breach was identified, Capital One and AWS took immediate action: revoking the compromised credentials, shutting down the vulnerable instance and notifying the authorities, notably the FBI. This incident underlined the importance of strict access controls and reinforced protection against SSRF attacks, which are now better controlled in cloud environments.\niCloud (2014), # On August 31, 2014, a massive leak of private celebrity photos flooded Reddit. These intimate images, stolen from some of the most high-profile actresses, models, singers, and athletes, quickly spread across the internet. The leak, dubbed The Fappening, gained immediate traction, with a subreddit dedicated to the images amassing over 100,000 followers in just 24 hours. Despite efforts to contain the spread, copies of the photos appeared on underground forums and other websites, dominating headlines for weeks.\nInvestigations into the hack revealed that private photos had initially surfaced on the imageboard 4chan and the obscure forum Anon-IB, both known for anonymously sharing explicit content. On August 26, 2014, an Anon-IB user claimed that members of a private group had accessed and obtained nude photos of Jennifer Lawrence, among other celebrities. Initially, the stolen photos were traded or sold in underground communities, with collectors amassing images in hopes of profiting. However, on August 31, someone decided to release the entire collection publicly on 4chan, igniting the scandal.\nFurther digging led to a secretive group within Anon-IB called Stol, consisting of several men across the United States. These individuals, ranging from a teacher to a bank teller, engaged in hacking and trading stolen private images, primarily targeting celebrities. Their methods involved phishing attacks, tricking victims into revealing their Apple iCloud credentials, granting them access to personal photo libraries.\nThe hack exposed major vulnerabilities in cloud security, prompting Apple to strengthen iCloud protections. Multiple individuals were eventually identified and prosecuted for their roles in the breach, though the damage had already been done. The scandal remains one of the most infamous digital privacy breaches in history.\nCloudBleed (2017)\nOn February 18, 2017, at 4:11 PM PST, a security researcher from Google's Project Zero discovered a critical vulnerability in Cloudflare's infrastructure. The issue was severe enough that he immediately reached out to Cloudflare, and by 4:32 PM, the company was made aware of a possible widespread data leak.\nCloudflare, a major Content Delivery Network (CDN) provider, had been unknowingly exposing sensitive customer data, including cookies, passwords, HTTPS requests, and private keys. This data leakage occurred due to a bug in how Cloudflare's edge servers processed HTML content. Worse, search engines like Google had already cached some of the leaked data, making it publicly accessible.\nBy 4:40 PM, Cloudflare's incident response team mobilized, including engineers from both San Francisco and London. Initial investigation suggested the bug was linked to Cloudflare\u0026rsquo;s email obfuscation feature, which had recently undergone modifications. Engineers swiftly disabled this feature via a global kill switch at 5:22 PM PST, but the issue persisted.\nFurther debugging identified two additional problematic features: automatic HTTP rewrites and server-side excludes. While automatic HTTP rewrites could be turned off immediately, server-side excludes lacked a kill switch, requiring a patch. By 11:22 PM PST, after hours of effort, a fix was finally deployed, preventing further data leakage.\nVulnerabilities # Risks of misconfigured cloud settings and unauthorized access. # Threats to Service Reliability # Impact of high-profile outages (e.g., AWS 2012 downtime). # Mitigating risks of third-party dependencies. # # "},{"id":3,"href":"/docs/emerging_technologies__associated_risks/","title":"Emerging Technologies \u0026 Associated Risks","section":"Docs","content":" Emerging Technologies \u0026amp; Associated Risks # Containerization \u0026amp; Microservices # Security implications of Docker (2013) and Kubernetes (2016). # Managing vulnerabilities in distributed systems. # Serverless \u0026amp; AI-Driven Environments # Risks in serverless computing (AWS Lambda) # Exposure of APIs and data pipelines. # # "},{"id":4,"href":"/docs/identity__access_management_iam/","title":"Identity \u0026 Access Management ( Iam)","section":"Docs","content":" Identity \u0026amp; Access Management (IAM) # Authentication \u0026amp; Authorization # Lessons from breaches tied to weak IAM practices. # Role of Zero Trust frameworks in cloud environments. # Privilege Escalation Risks # Case study: Capital One breach (2019). # # "},{"id":5,"href":"/docs/infrastructure__architectural_security/","title":"Infrastructure \u0026 Architectural Security","section":"Docs","content":" Infrastructure \u0026amp; Architectural Security # Shared Responsibility Models # Évolution of IaaS (EC2, S3), PaaS (Google App Engine), and SaaS (Salesforce). # Security gaps in provider vs. user obligations. # Hybrid \u0026amp; Multi-Cloud Complexity # Balancing control and flexibility in hybrid architectures (e.g., Azure, IBM SmartCloud). # Risks of vendor lock-in and interoperability challenges. # "},{"id":6,"href":"/docs/introduction_/","title":"Introduction","section":"Docs","content":" Introduction # Cloud infrastructure refers to a framework that allows businesses to access computing resources such as servers, storage, and applications over the internet, distinctively differentiating it from traditional on-premise systems. This shift to cloud-based solutions has gained prominence due to the myriad benefits it offers, including :\nEnhanced flexibility,\nScalability,\nCost efficiency,\nThe fundamental properties of cloud infrastructure encompass several key features:\nOn-demand self-service\nBroad network access\nResource pooling\nRapid elasticity\nMeasured service\nAbstraction and Simplification\nBackup and Disaster Recovery\nSelf-service allows users to provision resources autonomously, while broad network access ensures connectivity from diverse devices. Resource pooling provides a shared infrastructure for multiple clients, and rapid elasticity enables dynamic scaling according to fluctuating workloads. Furthermore, the measured service model offers detailed monitoring of resource usage, enhancing cost management and operational optimization. Lastly, cloud infrastructure provides a level of abstraction and simplification reducing the technical expertise required for systems management, while backup and disaster recovery are also simplified, providing levels of redundancy and reliability that is often more robust than traditional DR solutions.\nDespite its advantages, the transition to cloud infrastructure is not without challenges. Concerns surrounding data security, compliance, and potential vendor lock-in remain prominent. Security vulnerabilities inherent to shared resources and the need for robust compliance frameworks can deter businesses from fully embracing cloud solutions.\n"},{"id":7,"href":"/docs/key_milestones__successes__and_challenges/","title":"Key Milestones Successes and Challenges","section":"Docs","content":" Key Milestones, Successes, and Challenges # Early Concepts and Foundations (1960s\u0026ndash;1990s) # Cloud computing, as a concept, can trace its roots back to the 1960s when computer scientist John McCarthy proposed the idea of utility computing, envisioning a future where computing power could be offered as a service similar to electricity or water. This foundational idea set the stage for what would later evolve into modern cloud services. One of the early implementations of shared computing resources was IBM and MIT's Compatible Time-Sharing System (CTSS) developed in the early 1960s, which allowed multiple users to access a mainframe computer simultaneously.\nDuring the 1960s and 1970s, IBM made significant advancements in virtualization technology, developing the CP-67 and VM/370 operating systems that enabled multiple operating systems to run on a single physical machine. This ability to share computing resources efficiently was crucial for the development of cloud computing. Virtualization would become a fundamental aspect of cloud architecture, allowing for the scalable allocation of resources as needed.\nThe term 'cloud' itself became synonymous with distributed computing during the 1990s, largely due to the increasing use of the internet and early remote services like file sharing and email, which highlighted the potential for accessing resources over a network.\n2000\u0026ndash;2005: Establishing the Foundations # The early 2000s marked the transition from traditional software distribution to cloud-based solutions. In 1999, Salesforce pioneered Software as a Service (SaaS), delivering customer relationship management (CRM) tools via the web rather than on-premise installations. This demonstrated the feasibility of enterprise software hosted in the cloud.\nAmazon entered the cloud space in 2002 with the launch of Amazon Web Services (AWS), initially offering basic infrastructure services that laid the groundwork for cloud scalability. Meanwhile, in 2004, Google introduced Gmail, showcasing the potential of cloud-based consumer applications by offering scalable storage and remote accessibility.\n2006\u0026ndash;2010: The Emergence of Modern Cloud Computing # A pivotal moment in cloud computing came in 2006 with AWS's launch of Elastic Compute Cloud (EC2) and Simple Storage Service (S3). These services introduced the concept of Infrastructure as a Service (IaaS), enabling businesses to provision computing resources on demand. The National Institute of Standards and Technology (NIST) formally defined the three primary cloud service models: Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS), providing businesses with varying levels of abstraction and management responsibilities.\nThe following years saw further innovation in cloud platforms. In 2008, Google introduced Google App Engine, providing a Platform as a Service (PaaS) solution for developers to build and deploy applications without managing infrastructure. By 2010, Microsoft had entered the cloud market with Azure, positioning itself as a strong competitor to AWS and Google by emphasizing hybrid cloud solutions.\nDuring this time, enterprises rapidly adopted cloud computing, drawn by its cost efficiency and scalability, while SaaS applications, including Google Docs (launched in 2006) and Dropbox, gained widespread traction. However, challenges such as security and reliability concerns hindered adoption in regulated industries. Limited bandwidth and latency issues also impacted performance for cloud applications.\n2011\u0026ndash;2015: Expansion and Specialization # The early 2010s saw cloud computing evolve beyond basic storage and computing. In 2011, IBM introduced SmartCloud, targeting enterprises with hybrid cloud solutions. The adoption of VMware\u0026rsquo;s virtualization technologies facilitated private cloud deployments.\nA breakthrough in cloud-native application development came in 2013 with the rise of Docker, which popularized containerization. This innovation simplified application deployment across cloud environments, laying the foundation for modern microservices architectures.\nHowever, as cloud adoption grew, so did security concerns. The 2014 iCloud data breaches underscored vulnerabilities in cloud storage services, momentarily eroding user trust. While hybrid cloud strategies were gaining traction, balancing flexibility with on-premise control, Netflix completed its migration to AWS, demonstrating the cloud\u0026rsquo;s scalability for high-demand services.\nMeanwhile, high-profile service outages, such as AWS's 2012 downtime, raised concerns about cloud reliability. Furthermore, vendor lock-in emerged as an issue, with organizations struggling to migrate workloads between providers.\n2016\u0026ndash;2020: Maturation and Technological Breakthroughs # By the mid-2010s, cloud computing had matured into an essential component of enterprise IT strategies. In 2016, Kubernetes became the industry standard for container orchestration, further streamlining cloud-native development. Serverless computing gained traction in 2017, with services like AWS Lambda allowing developers to execute code without provisioning infrastructure.\nMulti-cloud strategies surged in 2019 as organizations sought to reduce reliance on a single provider and enhance resilience. Cloud providers also integrated AI and machine learning, enhancing analytics and automation. Additionally, edge computing emerged as a complement to centralized cloud infrastructure, reducing latency for critical applications.\nSecurity vulnerabilities were exposed by the Capital One data breach (2019), caused by misconfigured AWS settings. Meanwhile, rising operational costs led to 23% budget overruns, highlighting the need for better resource management.\n2021\u0026ndash;2024: AI, Sustainability, and Complexity # The 2020s have been defined by the increasing convergence of AI, cloud computing, and hybrid cloud architectures. By 2021, 87% of enterprises had adopted hybrid cloud strategies, leveraging both public and private infrastructure. Generative AI models, such as ChatGPT, fueled demand for GPU-powered cloud infrastructure, straining cloud provider resources.\nSimultaneously, cloud providers prioritized sustainability, with major firms reaching carbon-neutral milestones by 2024. AI-driven FinOps emerged, optimizing cloud costs through intelligent automation, while industry-specific cloud platforms for healthcare and finance improved regulatory compliance.\nMajor challenges remain as skills shortages persist, with organizations citing expertise gaps in cloud governance. Data sovereignty regulations have also prompted some businesses to migrate workloads back on-premise.\nHistorical Successes # A significant number of case studies highlight the successful implementation of Amazon Web Services (AWS) by various organizations. For instance, companies like Netflix have utilized AWS's machine learning services to personalize user experiences, leading to higher customer satisfaction and retention rates. Similarly, Airbnb, a leading online marketplace, adopted AWS to handle its massive data requirements, benefiting from the robust scalability and flexibility that cloud infrastructure provides.\n# "},{"id":8,"href":"/docs/operational_resilience__cost_management/","title":"Operational Resilience \u0026 Cost Management","section":"Docs","content":" Operational Resilience \u0026amp; Cost Management # FinOps \u0026amp; Cost Optimization # Addressing budget overruns through FinOps. # Tools for monitoring and optimizing cloud spend. # Disaster Recovery Strategies # Building redundancy in multi-cloud architectures (e.g., Netflix migration to AWS). # # High level security considerations. # Cloud infrastructures present unique security challenges. The increased exposure associated with cloud services can lead to a higher risk of cyberattacks and human error. The multitude of interactions within cloud environments heightens the potential for insider threats and operational mistakes, necessitating robust security measures and constant monitoring; Furthermore, while many cloud platforms tout advanced security features, organizations must ensure these are properly implemented to protect sensitive data effectively\n.\n.\n"},{"id":9,"href":"/docs/practical_application/","title":"Practical Application","section":"Docs","content":" Practical application # Objective # Our goal is to create a standard cloud infrastructure, and leverage our understanding to highlight and exploit the vulnerabilities we would have left behind.\nSetting up authorisations # We start by creating a google cloud project :\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;1.9895833333333333in\u0026rdquo;}\nWe proceed to the IAM section to create a service account for managing the IAM policies.\n{width=\u0026ldquo;3.7857141294838144in\u0026rdquo; {width=\u0026ldquo;3.6875in\u0026rdquo; height=\u0026ldquo;2.4843755468066493in\u0026rdquo;} height=\u0026ldquo;2.8333333333333335in\u0026rdquo;}\nOnce the service account is created we can export its credentials file, which will be the key to automating all the remaining configuration with IaC.\nAll the configuration is referenced under in this repo : [https://github.com/Aitbytes/Projet-Long-Infra/]{.underline}\nThe first step consists in creating a separate service account, then providing it with the necessary roles for further deployments tasks. The script used for that end is referenced under : [https://github.com/Aitbytes/Projet-Long-Infra/tree/main/PrepareAccounts]{.underline}\nSimultaneously it provides each team mate with the identical rôles.\nThe roles granted to both the service account and the teammates are :\nCloud Run Admin\nCloud Run Invoker\nCompute Admin\nKubernetes Engine Admin\nStorage Admin.\n"},{"id":10,"href":"/docs/virtualization/","title":"Virtualization","section":"Docs","content":" Virtualization # Analysis of various virtual machine attacks in cloud computing # Co-Resident Attack and its impact on Virtual Environment # An approach with two-stage mode to detect cache-based side channel attacks # Using Virtual Machine Allocation Policies to Defend against Co-Resident Attacks in Cloud Computing # In order to implement Resource pooling and multi-tenancy, cloud companies started using virtualization, allowing them to run multiple OS on the same computer. This technology is also the target of several attacks\nDoS Attacks : A hacker can use a cloud provider to perform a DDoS attack on the cloud provider itself or one of its clients.\nCo-Residential Attacks : Using side channels, a hacker can steal private information from virtual machines who share the same server as a malicious VM. But first, the hacker needs to co-locate one of his VMs with the target VM.\nCache-Based Side Channel Attacks : Possible for two VMs on the same computer using bare-metal hypervisor sharing highest level-cache\nHypervisor compromission : The hypervisor is the software responsible for virtualization. A compromised hypervisor may lead to the compromission of all hosted virtual machines.\nVM Escape : A hacker tries to exploit a fault in isolation features to gain access to the host machine.\nVM Cloning : A hacker copies the program files of a VM from the host OS and then makes a clone to access sensitive data.\nVM Memory Dump : A hacker dumps the memory assigned to the target VM to search for information.\n.Countermeasures\nTo mitigate the risk of co-resident attacks, researchers proposed various approaches\nPreventing Side Channel Attacks : Most SCA work with high resolution internal clocks. Eliminating references to internal clocks will protect software from several SCA. This solution however is hardware-based and costly\nPeriodic Migration : Regular migration following a VCG mechanism will hinder the capacity of an attacker to perform co-residential attacks. Drawbacks of this method include performance degradation and power consumption.\nPreventing Co-Residency Verification : To verify co-residency, attackers check if their VMs and the target VM share the same Dom0 IP address. If the Dom0 IP address is confined, a malicious user will have to resort to difficult techniques if he wants to confirm co-residency.\nDetecting Co-location attacks : Various features can be observed for anomalies in order to detect Cache-Based Side Channel Attacks, including CPU usage, RAM usage and cache miss.\nAllocation Policy : An allocation policy is the policy employed by a cloud provider to allocate VMs to a physical resource. Some policies can increase the number of attempts an attacker needs to achieve a co-residency. For example, CLR (Co-location Resistance Algorithm) opens a fixed number of servers and randomly allocates a VM, or PSSF (Previously-selected-server-first) will try to allocate a user\u0026rsquo;s VM to a server already used by the user.\n"}]